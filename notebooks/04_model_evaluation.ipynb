{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Evaluation\n",
        "\n",
        "This notebook evaluates trained OCR models and compares with Azure DI baseline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.serving.model_loader import load_model_for_inference\n",
        "from src.evaluation.evaluate_ocr import evaluate_from_file\n",
        "from src.evaluation.compare_models import compare_from_file\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load trained model\n",
        "model = load_model_for_inference(\n",
        "    det_model_path=\"../trained_models/detection_best.pth\",\n",
        "    reco_model_path=\"../trained_models/recognition_best.pth\"\n",
        ")\n",
        "\n",
        "# Evaluate on test set\n",
        "test_data_path = \"../data/evaluation_data/test.json\"\n",
        "metrics = evaluate_from_file(model, test_data_path)\n",
        "\n",
        "print(\"Evaluation Metrics:\")\n",
        "print(json.dumps(metrics, indent=2))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare with Azure DI\n",
        "comparison = compare_from_file(model, test_data_path)\n",
        "\n",
        "print(\"Comparison Results:\")\n",
        "print(json.dumps(comparison, indent=2))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
